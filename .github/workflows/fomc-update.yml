import requests
from bs4 import BeautifulSoup
import json, csv
from datetime import datetime

# --- æ ¸å¿ƒé…ç½® ---
TARGET_URL = "https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
OUTPUT_JSON = "fomc_calendar.json"
OUTPUT_CSV = "fomc_calendar.csv"
OUTPUT_TXT = "fomc_dates.txt"
# -----------------

def fetch_and_parse():
    """ä¸»æŠ“å–ä¸è§£æå‡½æ•°"""
    print(f"æ­£åœ¨è®¿é—®: {TARGET_URL}")
    try:
        resp = requests.get(TARGET_URL, headers=HEADERS, timeout=25)
        resp.raise_for_status()
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None

    # --- æ–°å¢ï¼šä¿å­˜ç½‘é¡µå¿«ç…§ç”¨äºè°ƒè¯• ---
    debug_html_path = "debug_fomc_page.html"
    with open(debug_html_path, "w", encoding="utf-8") as f:
        f.write(resp.text)
    print(f"ğŸ’¾ å·²å°†é¡µé¢HTMLä¿å­˜è‡³ {debug_html_path}ï¼Œç”¨äºåˆ†æç»“æ„")
    # ---------------------------------

    soup = BeautifulSoup(resp.content, 'lxml')
    meetings = []

    # --- å…³é”®ï¼šå¤šç§é€‰æ‹©å™¨ç­–ç•¥ï¼Œæé«˜å®¹é”™ç‡ ---
    # ç­–ç•¥ä¸€ï¼šå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«â€œpanel panel-defaultâ€çš„divï¼ˆè¿™æ˜¯å®˜ç½‘å†å²ä½¿ç”¨çš„å¹´ä»½æ¿å—å®¹å™¨ï¼‰
    year_panels = soup.find_all('div', class_='panel panel-default')
    
    print(f"ğŸ” æ‰¾åˆ°äº† {len(year_panels)} ä¸ªç–‘ä¼¼â€˜å¹´ä»½é¢æ¿â€™(panel panel-default)çš„åŒºå—ã€‚")
    
    for panel in year_panels:
        # åœ¨æ¿å—å†…æ‰¾å¹´ä»½
        year_title = panel.find('h4', class_='panel-title')
        if not year_title:
            print("  âš ï¸ åœ¨ä¸€ä¸ªé¢æ¿ä¸­æœªæ‰¾åˆ°å¹´ä»½æ ‡é¢˜(h4.panel-title)ï¼Œè·³è¿‡ã€‚")
            continue
        # ä»æ ‡é¢˜æ–‡æœ¬ä¸­æå–å¹´ä»½æ•°å­—
        year_text = ''.join(filter(str.isdigit, year_title.get_text()))
        if not year_text:
            print(f"  âš ï¸ æ— æ³•ä»æ ‡é¢˜æ–‡æœ¬â€˜{year_title.get_text()}â€™ä¸­æå–å¹´ä»½ï¼Œè·³è¿‡ã€‚")
            continue
        current_year = int(year_text)
        print(f"  ğŸ—“ï¸  æ­£åœ¨å¤„ç† {current_year} å¹´çš„æ•°æ®...")

        # åœ¨æ¿å—å†…æ‰¾ä¼šè®®è¡¨æ ¼
        table = panel.find('table')
        if not table:
            print(f"    âš ï¸ åœ¨{current_year}å¹´çš„é¢æ¿ä¸­æœªæ‰¾åˆ°è¡¨æ ¼(table)ï¼Œè·³è¿‡ã€‚")
            continue

        # è§£æè¡¨æ ¼è¡Œ
        rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´è¡Œ
        print(f"    åœ¨è¯¥è¡¨æ ¼ä¸­æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®ã€‚")
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 2:
                continue
            
            # ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸï¼Œç¬¬äºŒåˆ—æ˜¯ä¼šè®®ç±»å‹
            date_cell_text = cols[0].get_text(strip=True)
            meeting_type = cols[1].get_text(strip=True)
            
            print(f"      å‘ç°ä¼šè®®æ¡ç›®: æ—¥æœŸæ–‡æœ¬â€˜{date_cell_text}â€™ï¼Œ ç±»å‹â€˜{meeting_type}â€™")
            
            # *** ç®€åŒ–å¤„ç†ï¼šæš‚æ—¶åªè®°å½•åŸå§‹æ–‡æœ¬ï¼Œä¸è¿›è¡Œå¤æ‚çš„æ—¥æœŸè§£æ ***
            # å¾…è·å–debugæ–‡ä»¶åï¼Œå†é’ˆå¯¹æ€§ç¼–å†™è§£æé€»è¾‘
            start_date = f"{current_year}-æœªè§£æ" 
            end_date = f"{current_year}-æœªè§£æ"
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºSEPä¼šè®®
            is_sep = "summary of economic projections" in meeting_type.lower()
            
            meetings.append({
                "start_date": start_date,
                "end_date": end_date,
                "summary": meeting_type,
                "is_sep": is_sep,
                "has_press": is_sep,
                "source_url": TARGET_URL,
                "scraped_at": datetime.now().isoformat(),
                "åŸå§‹æ—¥æœŸæ–‡æœ¬": date_cell_text, # ä¿ç•™åŸå§‹æ–‡æœ¬ä¾›åˆ†æ
                "æ•°æ®çŠ¶æ€": "å¾…è§£æ"
            })

    print(f"âœ… åˆæ­¥è§£æåˆ° {len(meetings)} ä¸ªä¼šè®®æ¡ç›®ï¼ˆæ—¥æœŸå¾…è§£æï¼‰")
    return meetings

def save_data(meetings):
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    if not meetings:
        print("âš ï¸  æœªè·å–åˆ°æ•°æ®ï¼Œå°†åˆ›å»ºç©ºæ–‡ä»¶ã€‚")
        meetings = []

    # 1. ä¿å­˜ä¸ºJSON
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump({
            "meta": {
                "last_updated": datetime.now().isoformat(),
                "source": TARGET_URL,
                "count": len(meetings),
                "å¤‡æ³¨": "æ­¤ç‰ˆæœ¬ä¸ºè°ƒè¯•ç‰ˆï¼Œæ—¥æœŸå­—æ®µå°šæœªæ­£ç¡®è§£æï¼Œè¯·æä¾›åŒç›®å½•ä¸‹çš„debug_fomc_page.htmlæ–‡ä»¶ä»¥ç»§ç»­å¼€å‘ã€‚"
            },
            "meetings": meetings
        }, f, indent=2, ensure_ascii=False)

    # 2. ä¿å­˜ä¸ºCSV
    if meetings:
        # åŒ…å«æ›´å¤šå­—æ®µä»¥ä¾¿åˆ†æ
        fieldnames = ["start_date", "end_date", "summary", "is_sep", "has_press", "åŸå§‹æ—¥æœŸæ–‡æœ¬", "æ•°æ®çŠ¶æ€"]
        with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(meetings)
    else:
        with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:
            f.write("start_date,end_date,summary,is_sep,has_press,åŸå§‹æ—¥æœŸæ–‡æœ¬,æ•°æ®çŠ¶æ€\n")

    # 3. ä¿å­˜ä¸ºçº¯æ–‡æœ¬TXT
    with open(OUTPUT_TXT, 'w', encoding='utf-8') as f:
        if meetings:
            for m in meetings:
                f.write(f"{m['åŸå§‹æ—¥æœŸæ–‡æœ¬']} ({m['summary']})\n")
        else:
            f.write("No meetings found.\n")

    print(f"ğŸ“ æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_JSON}, {OUTPUT_CSV}, {OUTPUT_TXT}")

# --- è„šæœ¬æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒFOMCæ—¥å†æŠ“å–ä»»åŠ¡")
    data = fetch_and_parse()
    save_data(data)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
