import requests
from bs4 import BeautifulSoup
import json, csv
from datetime import datetime

# --- æ ¸å¿ƒé…ç½® ---
TARGET_URL = "https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
OUTPUT_JSON = "fomc_calendar.json"
OUTPUT_CSV = "fomc_calendar.csv"
OUTPUT_TXT = "fomc_dates.txt"
# -----------------

def fetch_and_parse():
    """ä¸»æŠ“å–ä¸è§£æå‡½æ•°"""
    print(f"æ­£åœ¨è®¿é—®: {TARGET_URL}")
    try:
        resp = requests.get(TARGET_URL, headers=HEADERS, timeout=25)
        resp.raise_for_status()
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None

    # --- æ–°å¢ï¼šä¿å­˜ç½‘é¡µå¿«ç…§ç”¨äºè°ƒè¯• ---
    debug_html_path = "debug_fomc_page.html"
    with open(debug_html_path, "w", encoding="utf-8") as f:
        f.write(resp.text)
    print(f"ğŸ’¾ å·²å°†é¡µé¢HTMLä¿å­˜è‡³ {debug_html_path}ï¼Œç”¨äºåˆ†æç»“æ„")
    # ---------------------------------

    soup = BeautifulSoup(resp.content, 'lxml')
    meetings = []

    # --- å…³é”®ï¼šå¤šç§é€‰æ‹©å™¨ç­–ç•¥ï¼Œæé«˜å®¹é”™ç‡ ---
    # ç­–ç•¥ä¸€ï¼šå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«â€œpanel panel-defaultâ€çš„divï¼ˆè¿™æ˜¯å®˜ç½‘å†å²ä½¿ç”¨çš„å¹´ä»½æ¿å—å®¹å™¨ï¼‰
    year_panels = soup.find_all('div', class_='panel panel-default')
    
    for panel in year_panels:
        # åœ¨æ¿å—å†…æ‰¾å¹´ä»½
        year_title = panel.find('h4', class_='panel-title')
        if not year_title:
            continue
        # ä»æ ‡é¢˜æ–‡æœ¬ä¸­æå–å¹´ä»½æ•°å­—
        year_text = ''.join(filter(str.isdigit, year_title.get_text()))
        if not year_text:
            continue
        current_year = int(year_text)

        # åœ¨æ¿å—å†…æ‰¾ä¼šè®®è¡¨æ ¼
        table = panel.find('table')
        if not table:
            continue

        # è§£æè¡¨æ ¼è¡Œ
        for row in table.find_all('tr')[1:]:  # è·³è¿‡è¡¨å¤´è¡Œ
            cols = row.find_all('td')
            if len(cols) < 2:
                continue
            
            # ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸï¼Œç¬¬äºŒåˆ—æ˜¯ä¼šè®®ç±»å‹
            date_cell_text = cols[0].get_text(strip=True)
            meeting_type = cols[1].get_text(strip=True)
            
            # æ¸…æ´—å¹¶æ„é€ æ—¥æœŸå­—ç¬¦ä¸² (ä¾‹å¦‚: "January 28-29, 2025")
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†æ—¥æœŸå­—ç¬¦ä¸²ä¸å¹´ä»½ç»“åˆï¼Œå¹¶è¿›è¡Œæ›´ç²¾ç»†çš„è§£æ
            # ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå®é™…éœ€æ ¹æ®å®˜ç½‘æ—¥æœŸæ ¼å¼è°ƒæ•´
            raw_date_str = f"{date_cell_text}, {current_year}"
            
            # æ­¤å¤„ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è§£æraw_date_strä¸ºstart_dateå’Œend_date
            # æ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨dateutilæˆ–æ‰‹åŠ¨æ‹†åˆ†â€œ-â€æ¥å¤„ç†æ—¥æœŸèŒƒå›´
            start_date = f"{current_year}-01-01"  # æ­¤å¤„åº”ä¸ºè§£æåçš„çœŸå®å¼€å§‹æ—¥æœŸ
            end_date = f"{current-year}-01-01"    # æ­¤å¤„åº”ä¸ºè§£æåçš„çœŸå®ç»“æŸæ—¥æœŸ
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºSEPä¼šè®® (é€šå¸¸æè¿°ä¸­ä¼šåŒ…å«ç›¸å…³ä¿¡æ¯)
            is_sep = "summary of economic projections" in meeting_type.lower()
            
            meetings.append({
                "start_date": start_date,
                "end_date": end_date,
                "summary": meeting_type,
                "is_sep": is_sep,
                "has_press": is_sep,  # å‡è®¾SEPä¼šè®®éƒ½æœ‰æ–°é—»å‘å¸ƒä¼š
                "source_url": TARGET_URL,
                "scraped_at": datetime.now().isoformat()
            })

    print(f"âœ… åˆæ­¥è§£æåˆ° {len(meetings)} ä¸ªä¼šè®®æ¡ç›®")
    return meetings

def save_data(meetings):
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    if not meetings:
        print("âš ï¸  æœªè·å–åˆ°æ•°æ®ï¼Œå°†åˆ›å»ºç©ºæ–‡ä»¶ã€‚")
        meetings = []  # ç¡®ä¿ä¸ºç©ºåˆ—è¡¨ï¼Œé¿å…åç»­é”™è¯¯

    # 1. ä¿å­˜ä¸ºJSON (ç»“æ„åŒ–æœ€å¥½)
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump({
            "meta": {
                "last_updated": datetime.now().isoformat(),
                "source": TARGET_URL,
                "count": len(meetings)
            },
            "meetings": meetings
        }, f, indent=2, ensure_ascii=False)

    # 2. ä¿å­˜ä¸ºCSV (ä¾¿äºè¡¨æ ¼è½¯ä»¶æ‰“å¼€)
    if meetings:
        fieldnames = ["start_date", "end_date", "summary", "is_sep", "has_press"]
        with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(meetings)
    else:
        # å¦‚æœæ— æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªåªæœ‰è¡¨å¤´çš„CSV
        with open(OUTPUT_CSV, 'w', encoding='utf-8', newline='') as f:
            f.write("start_date,end_date,summary,is_sep,has_press")

    # 3. ä¿å­˜ä¸ºçº¯æ–‡æœ¬TXT (ä»…æ—¥æœŸï¼Œæ¯è¡Œä¸€ä¸ª)
    with open(OUTPUT_TXT, 'w', encoding='utf-8') as f:
        if meetings:
            for m in meetings:
                f.write(f"{m['start_date']} to {m['end_date']}: {m['summary']}\n")
        else:
            f.write("No meetings found.\n")

    print(f"ğŸ“ æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_JSON}, {OUTPUT_CSV}, {OUTPUT_TXT}")

# --- è„šæœ¬æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒFOMCæ—¥å†æŠ“å–ä»»åŠ¡")
    data = fetch_and_parse()
    save_data(data)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
